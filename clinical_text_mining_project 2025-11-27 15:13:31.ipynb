{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "415aac26-9261-447f-b2ab-67874618ed0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# 1. Defining the Schema (Column Names and Types)\n",
    "schema = StructType([\n",
    "    StructField(\"Patient_ID\", IntegerType(), True),\n",
    "    StructField(\"Discharge_Summary\", StringType(), True)\n",
    "])\n",
    "\n",
    "# 2. Defining the Data\n",
    "data = [\n",
    "    (101, \"Patient discharged home. No concerns regarding immediate safety. Has stable housing and reliable transportation. Follow-up scheduled for 7 days.\"),\n",
    "    (102, \"Patient expressed difficulty affording medications due to recent job loss. History of tobacco use documented. Needs referral to social worker for housing assistance.\"),\n",
    "    (103, \"Discharged with complex medication regimen. Patient lives alone and reported occasional heavy alcohol use. Recommend community resources for social support.\"),\n",
    "    (104, \"Routine discharge. Patient is a non-smoker with good family support structure. Education provided on diet and exercise.\"),\n",
    "    (105, \"Readmitted within 48 hours. Discharge note indicated patient was homeless and unable to contact for follow-up. Substance abuse history noted.\")\n",
    "]\n",
    "\n",
    "# 3. Creating the Spark DataFrame\n",
    "df_spark_raw = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "print(\"Schema of the created data:\")\n",
    "df_spark_raw.printSchema()\n",
    "df_spark_raw.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b60cc8b0-30e5-4304-8022-ba3d49686ea1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Defining keyword dictioning \n",
    "\n",
    "risk_keywords = {\n",
    "    'Housing_Risk': ['homeless', 'housing assistance', 'lives alone', 'unstable housing'],\n",
    "    'Employment_Financial_Risk': ['job loss', 'unemployment', 'unable to afford medications', 'difficulty affording', 'financial strain', 'cost issues'], \n",
    "    'Substance_Use_Risk': ['alcohol use', 'substance abuse', 'tobacco use', 'heavy smoking']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a97db178-aa4c-4ff7-924f-a27e5aeb8757",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from pyspark.sql.functions import col, upper, regexp_replace, when, lit\n",
    "\n",
    "# Checking the text and creating new columns\n",
    "\n",
    "# 1. Clean and Uppercase the Summary Column (Case-Insensitive Matching)\n",
    "# Using 'withColumn' method to create a new column\n",
    "df_spark_processed = df_spark_raw.withColumn(\n",
    "    \"Summary_Upper\", \n",
    "    upper(col(\"Discharge_Summary\"))\n",
    ")\n",
    "\n",
    "# 2. Iterating and Creating Binary Flags (1 or 0)\n",
    "for risk_category, keywords in risk_keywords.items():\n",
    "    \n",
    "    # A. Creating the Pipe-Separated Regex Pattern (e.g., 'HOMELESS|JOB LOSS...')\n",
    "    # Using 'join' to combine the list of keywords into a single string, separated by '|'.\n",
    "    pattern = '|'.join([k.upper() for k in keywords])\n",
    "\n",
    "    # B. Creating the new binary column using 'when' and 'rlike' (PySpark's Regex search)\n",
    "    # Using 'when' for conditional logic (IF...THEN...ELSE).\n",
    "    df_spark_processed = df_spark_processed.withColumn(\n",
    "        risk_category,\n",
    "        when(\n",
    "            col(\"Summary_Upper\").rlike(pattern),  # IF the text matches the pattern (rlike is regex-like)\n",
    "            lit(1)                               # THEN set the value to 1\n",
    "        ).otherwise(\n",
    "            lit(0)                               # ELSE set the value to 0\n",
    "        )\n",
    "    )\n",
    "\n",
    "# 3. Select Final Structured Data and Print\n",
    "df_structured = df_spark_processed.select(\n",
    "    \"Patient_ID\", \n",
    "    \"Discharge_Summary\", \n",
    "    \"Housing_Risk\", \n",
    "    \"Employment_Financial_Risk\", \n",
    "    \"Substance_Use_Risk\"\n",
    ")\n",
    "\n",
    "print(\"--- Extracted Structured Risk Data (PySpark) ---\")\n",
    "df_structured.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d02ff1c5-64d1-4d90-b4eb-4b7e8ff8b4e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum as spark_sum, col\n",
    "\n",
    "# 1. Getting total number of patients (total rows in the DataFrame)\n",
    "total_patients = df_structured.count()\n",
    "\n",
    "# 2. Performing the aggregation: Summing the 1s for each risk column (spark_sum)\n",
    "risk_counts_df = df_structured.agg(\n",
    "    spark_sum(col(\"Housing_Risk\")).alias(\"Housing_Count\"),\n",
    "    spark_sum(col(\"Employment_Financial_Risk\")).alias(\"Employment_Financial_Count\"),\n",
    "    spark_sum(col(\"Substance_Use_Risk\")).alias(\"Substance_Use_Count\")\n",
    ")\n",
    "\n",
    "print(f\"Total Patients Analyzed: {total_patients}\")\n",
    "print(\"Raw Counts of Risk Factors:\")\n",
    "risk_counts_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1c51e9c-d149-4422-ad77-3abf10a79f99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Pull the single row of counts back to the driver for simpler calculation\n",
    "risk_counts_row = risk_counts_df.collect()[0]\n",
    "\n",
    "# 2. Extract counts and calculate rates (Count / Total * 100)\n",
    "housing_rate = (risk_counts_row[\"Housing_Count\"] / total_patients) * 100\n",
    "employment_rate = (risk_counts_row[\"Employment_Financial_Count\"] / total_patients) * 100\n",
    "substance_rate = (risk_counts_row[\"Substance_Use_Count\"] / total_patients) * 100\n",
    "\n",
    "print(f\"\\n--- Final SDOH Risk Prevalence Rates ---\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"1. Housing Risk Prevalence: {housing_rate:.2f}%\")\n",
    "print(f\"2. Employment/Financial Risk Prevalence: {employment_rate:.2f}%\")\n",
    "print(f\"3. Substance Use Risk Prevalence: {substance_rate:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fc0d70b-9701-48f8-b9b8-376c01aeabca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Structure the results for visualization\n",
    "summary_data = [\n",
    "    (\"Housing Risk\", housing_rate),\n",
    "    (\"Employment/Financial Risk\", employment_rate),\n",
    "    (\"Substance Use Risk\", substance_rate)\n",
    "]\n",
    "\n",
    "# Create a final Pandas DataFrame for presentation\n",
    "summary_df_pandas = pd.DataFrame(summary_data, columns=['Risk_Category', 'Prevalence_Rate'])\n",
    "\n",
    "# Use the Databricks-specific display command to show the results table and automatically chart it!\n",
    "display(summary_df_pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2da936c-d8df-470a-9d35-1be2298b4024",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Using Pandas DataFrame created in the previous step\n",
    "fig = px.bar(\n",
    "    summary_df_pandas, \n",
    "    x='Risk_Category', \n",
    "    y='Prevalence_Rate',\n",
    "    title='SDOH Risk Prevalence Rates',\n",
    "    color='Prevalence_Rate', \n",
    "    color_continuous_scale=px.colors.sequential.Viridis,\n",
    "    text=summary_df_pandas['Prevalence_Rate'].apply(lambda x: f'{x:.1f}%') \n",
    ")\n",
    "\n",
    "# Customizing the layout for clarity\n",
    "fig.update_traces(textposition='outside')\n",
    "fig.update_layout(xaxis_title='Risk Factor',\n",
    "                  yaxis_title='Prevalence Rate (%)',\n",
    "                  title_x=0.5)\n",
    "\n",
    "# Using Databricks display() function on the Plotly figure object\n",
    "display(fig)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "clinical_text_mining_project 2025-11-27 15:13:31",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
